[tokenizer]
merges_path = "../diffusionLM/data/gpt2_merges.txt"
vocab_path = "../diffusionLM/data/gpt2_vocab.json"
special_tokens = ["<|pad|>", "<|eos|>"]

[model]
vocab_size = 50257
context_length = 256
d_model = 512
num_layers = 4
num_heads = 16
d_ff = 1344
rope_theta = 10000.0
device = "cuda"       # or "cpu"
dtype = "float32"     # "float32" | "float16" | "bfloat16"

[checkpoint]
ckpt_path = "./runs/2025-08-04_00-16-58_28jyumc2/4000.ckpt"

[inference]
text_list = [
  "Once upon a time,",
  "In a distant galaxy,",
]
temperature = 1.0
p = 0.9
eos_token_id = 1

[benchmark]
warmup = 5
repeats = 10
steps = 64
synchronize = true
backward = true
optimizer_step = true

[logging]
backend = "console"
run_name = ""
architecture = "TransformerLM"
dataset = "synthetic"

# Optional optimizer section for training-step benchmarking
[optimizer]
lr = 0.0
betas = [0.9, 0.999]
eps = 1e-8
weight_decay = 0.0
grad_clip_max_l2_norm = 0.0
