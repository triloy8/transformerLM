[input]
input_filename = "./data/TinyStoriesV2-GPT4-valid.txt"
total_tokens = 5535291

[output]
output_filename = "./data/test.dat"

[tokenizer]
vocab_path = "./data/gpt2_vocab.json"
merges_path = "./data/gpt2_merges.txt"
special_tokens = ["<|endoftext|>"]
